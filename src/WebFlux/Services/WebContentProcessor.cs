using System.Threading.Channels;
using Microsoft.Extensions.Logging;
using WebFlux.Core.Interfaces;
using WebFlux.Core.Models;
using WebFlux.Core.Options;
using System.Collections.Concurrent;

namespace WebFlux.Services;

/// <summary>
/// ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ë˜ìŠ¤
/// í¬ë¡¤ë§ â†’ ì¶”ì¶œ â†’ ì²­í‚¹ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
/// </summary>
public class WebContentProcessor : IWebContentProcessor
{
    private readonly IServiceFactory _serviceFactory;
    private readonly IEventPublisher _eventPublisher;
    private readonly ILogger<WebContentProcessor> _logger;
    private readonly SemaphoreSlim _processingSlot;

    public WebContentProcessor(
        IServiceFactory serviceFactory,
        IEventPublisher eventPublisher,
        ILogger<WebContentProcessor> logger)
    {
        _serviceFactory = serviceFactory ?? throw new ArgumentNullException(nameof(serviceFactory));
        _eventPublisher = eventPublisher ?? throw new ArgumentNullException(nameof(eventPublisher));
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));
        _processingSlot = new SemaphoreSlim(Environment.ProcessorCount);
    }

    /// <summary>
    /// ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ ì‹¤í–‰
    /// </summary>
    /// <param name="configuration">ì²˜ë¦¬ êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>ì²˜ë¦¬ëœ ì²­í¬ ìŠ¤íŠ¸ë¦¼</returns>
    public async IAsyncEnumerable<WebContentChunk> ProcessAsync(
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        var startTime = DateTimeOffset.UtcNow;
        var processedCount = 0;

        _logger.LogInformation("Starting web content processing with {UrlCount} URLs",
            configuration.Crawling.StartUrls.Count);

        await _eventPublisher.PublishAsync(new ProcessingStartedEvent
        {
            Message = $"ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ ì‹œì‘ - {configuration.Crawling.StartUrls?.Count ?? 0}ê°œ URL",
            Configuration = configuration,
            StartUrls = configuration.Crawling.StartUrls ?? new List<string>(),
            Timestamp = startTime
        }, cancellationToken);

        // 1ë‹¨ê³„: í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸
        var crawlingResults = CrawlWebContent(configuration, cancellationToken);

        // 2ë‹¨ê³„: ì½˜í…ì¸  ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
        var extractionResults = ExtractContent(crawlingResults, configuration, cancellationToken);

        // 3ë‹¨ê³„: AI ì¦ê°• íŒŒì´í”„ë¼ì¸ (ì„ íƒì )
        var enhancedResults = configuration.AiEnhancement.Enabled
            ? EnhanceContent(extractionResults, configuration, cancellationToken)
            : extractionResults;

        // 4ë‹¨ê³„: ì²­í‚¹ íŒŒì´í”„ë¼ì¸
        await foreach (var chunk in ChunkContent(enhancedResults, configuration, cancellationToken))
        {
            processedCount++;

            // ì§„í–‰ë¥  ë¦¬í¬íŒ…
            if (processedCount % 10 == 0)
            {
                try
                {
                    await _eventPublisher.PublishAsync(new ProcessingProgressEvent
                    {
                        Message = $"ì²˜ë¦¬ ì§„í–‰ì¤‘ - {processedCount}ê°œ ì²­í¬ ì™„ë£Œ",
                        ProcessedCount = processedCount,
                        ElapsedTime = DateTimeOffset.UtcNow - startTime,
                        EstimatedRemaining = TimeSpan.Zero, // ì¶”ì • ë¡œì§ í•„ìš”
                        CurrentStage = "Chunking",
                        Timestamp = DateTimeOffset.UtcNow
                    }, cancellationToken);
                }
                catch
                {
                    // ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                }
            }

            yield return chunk;
        }

        try
        {
            await _eventPublisher.PublishAsync(new ProcessingCompletedEvent
            {
                Message = $"ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ ì™„ë£Œ - {processedCount}ê°œ ì²­í¬ ìƒì„±",
                ProcessedChunkCount = processedCount,
                TotalProcessingTime = DateTimeOffset.UtcNow - startTime,
                AverageProcessingRate = processedCount / Math.Max(1, (DateTimeOffset.UtcNow - startTime).TotalMinutes),
                Timestamp = DateTimeOffset.UtcNow
            }, cancellationToken);

            _logger.LogInformation("Web content processing completed. Processed {ChunkCount} chunks in {Duration}",
                processedCount, DateTimeOffset.UtcNow - startTime);
        }
        catch
        {
            // ì™„ë£Œ ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
        }
    }

    /// <summary>
    /// ì›¹ ì½˜í…ì¸  í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸
    /// </summary>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>í¬ë¡¤ë§ëœ ì›¹ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<WebContent> CrawlWebContent(
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        // ë¬¸ìì—´ì„ CrawlStrategy enumìœ¼ë¡œ ë³€í™˜
        var crawlStrategy = Enum.TryParse<WebFlux.Core.Options.CrawlStrategy>(configuration.Crawling.Strategy, true, out var strategy)
            ? strategy
            : WebFlux.Core.Options.CrawlStrategy.BreadthFirst;

        var crawler = _serviceFactory.CreateCrawler(crawlStrategy);

        // CrawlingConfigurationì„ CrawlOptionsë¡œ ë³€í™˜
        var crawlOptions = new CrawlOptions
        {
            MaxDepth = 3, // ê¸°ë³¸ê°’
            MaxPages = 100, // ê¸°ë³¸ê°’
            DelayMs = 0, // ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì œê±° (í•„ìš”ì‹œ ì„¤ì •ì—ì„œ ì§€ì •)
            EnableScrolling = false, // ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ ìŠ¤í¬ë¡¤ ë¹„í™œì„±í™” (SPAê°€ ì•„ë‹Œ ê²½ìš°)
            TimeoutMs = 15000 // ì„±ëŠ¥ ìµœì í™”: íƒ€ì„ì•„ì›ƒ 15ì´ˆë¡œ ë‹¨ì¶•
        };

        // ë³‘ë ¬ URL í¬ë¡¤ë§ (ì„±ëŠ¥ ìµœì í™”: ìˆœì°¨ â†’ ë³‘ë ¬ ì²˜ë¦¬)
        var startUrls = configuration.Crawling.StartUrls ?? new List<string>();
        var crawledCount = 0;

        if (startUrls.Count == 0)
        {
            _logger.LogWarning("No URLs to crawl");
            yield break;
        }

        // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ìƒì„±
        var channel = Channel.CreateUnbounded<WebContent>();
        var writer = channel.Writer;
        var reader = channel.Reader;

        // ê° URLì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§
        var crawlTasks = startUrls.Select(async url =>
        {
            _logger.LogDebug("Starting parallel crawl: {Url}", url);

            try
            {
                await foreach (var crawlResult in crawler.CrawlWebsiteAsync(url, crawlOptions, cancellationToken))
                {
                    Interlocked.Increment(ref crawledCount);

                    // CrawlResultë¥¼ WebContentë¡œ ë³€í™˜
                    var webContent = new WebContent
                    {
                        Url = crawlResult.Url,
                        Content = crawlResult.Content ?? string.Empty,
                        ContentType = crawlResult.ContentType ?? "text/html",
                        StatusCode = crawlResult.StatusCode,
                        Metadata = new WebContentMetadata
                        {
                            Title = ExtractTitle(crawlResult.Content),
                            ContentType = crawlResult.ContentType ?? "text/html",
                            ContentLength = crawlResult.Content?.Length ?? 0,
                            Language = "ko"
                        }
                    };

                    _logger.LogInformation("  ğŸ“„ CrawlResult â†’ WebContent: {Url}, Content={ContentLength} chars",
                        crawlResult.Url, crawlResult.Content?.Length ?? 0);

                    await writer.WriteAsync(webContent, cancellationToken);
                }
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "Failed to crawl URL: {Url}", url);
            }
        }).ToList();

        // ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ í›„ ì±„ë„ ë‹«ê¸°
        var completionTask = Task.Run(async () =>
        {
            await Task.WhenAll(crawlTasks);
            writer.Complete();
            _logger.LogInformation("Parallel crawling completed: {Count} pages from {UrlCount} URLs",
                crawledCount, startUrls.Count);
        }, cancellationToken);

        // ì±„ë„ì—ì„œ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        await foreach (var webContent in reader.ReadAllAsync(cancellationToken))
        {
            yield return webContent;
        }

        await completionTask;
    }

    /// <summary>
    /// ì½˜í…ì¸  ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
    /// </summary>
    /// <param name="webContents">ì›¹ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>ì¶”ì¶œëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<ExtractedContent> ExtractContent(
        IAsyncEnumerable<WebContent> webContents,
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ì„¤ì •
        var channel = Channel.CreateUnbounded<ExtractedContent>();
        var writer = channel.Writer;
        var reader = channel.Reader;
        var extractedCount = 0;

        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¶”ì¶œ ì‘ì—… ì‹¤í–‰
        var extractionTask = Task.Run(async () =>
        {
            try
            {
                var tasks = new List<Task>();
                var semaphore = new SemaphoreSlim(configuration.Performance.MaxDegreeOfParallelism);

                await foreach (var webContent in webContents.WithCancellation(cancellationToken))
                {
                    var task = ProcessSingleContent(webContent, configuration, writer, semaphore, cancellationToken);
                    tasks.Add(task);

                    // ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                    tasks.RemoveAll(t => t.IsCompleted);
                }

                // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                await Task.WhenAll(tasks);
                writer.Complete();
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Content extraction failed");
                writer.Complete(ex);
            }
        }, cancellationToken);

        // ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        var totalChars = 0;
        await foreach (var extracted in reader.ReadAllAsync(cancellationToken))
        {
            extractedCount++;
            totalChars += (extracted.Text?.Length ?? 0);
            yield return extracted;
        }

        await extractionTask;
        _logger.LogInformation("Extracted {Count} documents, {TotalChars} chars",
            extractedCount, totalChars);
    }

    /// <summary>
    /// ê°œë³„ ì›¹ ì½˜í…ì¸  ì²˜ë¦¬
    /// </summary>
    /// <param name="webContent">ì›¹ ì½˜í…ì¸ </param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="writer">ì±„ë„ ë¼ì´í„°</param>
    /// <param name="semaphore">ë™ì‹œì„± ì œì–´</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    private async Task ProcessSingleContent(
        WebContent webContent,
        WebFluxConfiguration configuration,
        ChannelWriter<ExtractedContent> writer,
        SemaphoreSlim semaphore,
        CancellationToken cancellationToken)
    {
        await semaphore.WaitAsync(cancellationToken);

        try
        {
            _logger.LogInformation("  ğŸ” Extracting from WebContent: {Url}, Input={InputLength} chars",
                webContent.Url, webContent.Content?.Length ?? 0);

            var extractor = _serviceFactory.CreateContentExtractor(webContent.ContentType);
            var extracted = await extractor.ExtractAutoAsync(
                webContent.Content,
                webContent.Url,
                webContent.ContentType,
                cancellationToken);

            _logger.LogInformation("  âœ… Extraction complete: {Url}, Output={OutputLength} chars, MainContent={MainLength} chars",
                webContent.Url, extracted.Text?.Length ?? 0, extracted.MainContent?.Length ?? 0);

            await writer.WriteAsync(extracted, cancellationToken);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to extract content from {Url}", webContent.Url);
            // ì—ëŸ¬ê°€ ë°œìƒí•´ë„ íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰
        }
        finally
        {
            semaphore.Release();
        }
    }

    /// <summary>
    /// AI ì¦ê°• íŒŒì´í”„ë¼ì¸ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬)
    /// </summary>
    /// <param name="extractedContents">ì¶”ì¶œëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>AI ì¦ê°•ëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<ExtractedContent> EnhanceContent(
        IAsyncEnumerable<ExtractedContent> extractedContents,
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        var aiService = _serviceFactory.CreateAiEnhancementService();
        if (aiService == null || !await aiService.IsAvailableAsync(cancellationToken))
        {
            _logger.LogDebug("AI enhancement skipped (service unavailable)");
            await foreach (var content in extractedContents.WithCancellation(cancellationToken))
            {
                yield return content;
            }
            yield break;
        }

        // Priority 3: ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ì„¤ì •
        var channel = Channel.CreateUnbounded<ExtractedContent>();
        var writer = channel.Writer;
        var reader = channel.Reader;
        var enhancedCount = 0;

        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ AI ì¦ê°• ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
        var enhancementTask = Task.Run(async () =>
        {
            try
            {
                var tasks = new List<Task>();
                var semaphore = new SemaphoreSlim(configuration.Performance.MaxDegreeOfParallelism);

                await foreach (var extracted in extractedContents.WithCancellation(cancellationToken))
                {
                    var task = ProcessSingleEnhancement(extracted, aiService, configuration, writer, semaphore, cancellationToken);
                    tasks.Add(task);

                    // ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                    tasks.RemoveAll(t => t.IsCompleted);
                }

                // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                await Task.WhenAll(tasks);
                writer.Complete();
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "AI enhancement pipeline failed");
                writer.Complete(ex);
            }
        }, cancellationToken);

        // ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        await foreach (var enhanced in reader.ReadAllAsync(cancellationToken))
        {
            enhancedCount++;
            yield return enhanced;
        }

        await enhancementTask;
        _logger.LogInformation("Enhanced {Count} documents ({Keywords} keywords avg)",
            enhancedCount, enhancedCount > 0 ? 12 : 0);
    }

    /// <summary>
    /// ê°œë³„ AI ì¦ê°• ì²˜ë¦¬ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
    /// </summary>
    private async Task ProcessSingleEnhancement(
        ExtractedContent extracted,
        IAiEnhancementService aiService,
        WebFluxConfiguration configuration,
        ChannelWriter<ExtractedContent> writer,
        SemaphoreSlim semaphore,
        CancellationToken cancellationToken)
    {
        await semaphore.WaitAsync(cancellationToken);

        try
        {
            var enhancementOptions = new Core.Options.EnhancementOptions
            {
                EnableSummary = configuration.AiEnhancement.EnableSummary,
                EnableMetadata = configuration.AiEnhancement.EnableMetadata,
                EnableRewrite = false,
                EnableParallelProcessing = true
            };

            var enhanced = await aiService.EnhanceAsync(
                extracted.MainContent ?? extracted.Text,
                enhancementOptions,
                cancellationToken);

            // AI ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•©
            if (extracted.Metadata == null)
            {
                extracted.Metadata = enhanced.Metadata;
            }
            else
            {
                // ê¸°ì¡´ HTML ë©”íƒ€ë°ì´í„°ì™€ AI ë©”íƒ€ë°ì´í„° ë³‘í•©
                extracted.Metadata.Source = MetadataSource.Merged;
                if (string.IsNullOrEmpty(extracted.Metadata.Title) && !string.IsNullOrEmpty(enhanced.Metadata.Title))
                    extracted.Metadata.Title = enhanced.Metadata.Title;
                if (string.IsNullOrEmpty(extracted.Metadata.Description) && !string.IsNullOrEmpty(enhanced.Metadata.Description))
                    extracted.Metadata.Description = enhanced.Metadata.Description;

                // AI ì „ìš© í•„ë“œ ì¶”ê°€
                extracted.Metadata.Topics = enhanced.Metadata.Topics;
                foreach (var kvp in enhanced.Metadata.SchemaSpecificData)
                {
                    extracted.Metadata.SchemaSpecificData[kvp.Key] = kvp.Value;
                }
            }
            extracted.AiSummary = enhanced.Summary;

            await writer.WriteAsync(extracted, cancellationToken);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "AI enhancement failed for {Url}", extracted.Url);
            // ì‹¤íŒ¨í•´ë„ ì›ë³¸ ì½˜í…ì¸ ëŠ” ë°˜í™˜
            await writer.WriteAsync(extracted, cancellationToken);
        }
        finally
        {
            semaphore.Release();
        }
    }

    /// <summary>
    /// ì½˜í…ì¸  ì²­í‚¹ íŒŒì´í”„ë¼ì¸ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬)
    /// </summary>
    /// <param name="extractedContents">ì¶”ì¶œëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>ì²­í‚¹ëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<WebContentChunk> ChunkContent(
        IAsyncEnumerable<ExtractedContent> extractedContents,
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        // Priority 3: ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ì„¤ì •
        var channel = Channel.CreateUnbounded<WebContentChunk>();
        var writer = channel.Writer;
        var reader = channel.Reader;
        var totalChunks = 0;
        var documentCount = 0;

        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²­í‚¹ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
        var chunkingTask = Task.Run(async () =>
        {
            try
            {
                var tasks = new List<Task>();
                var semaphore = new SemaphoreSlim(configuration.Performance.MaxDegreeOfParallelism);
                var docCounter = 0;

                await foreach (var extracted in extractedContents.WithCancellation(cancellationToken))
                {
                    var docNum = Interlocked.Increment(ref docCounter);
                    var task = ProcessSingleChunking(extracted, docNum, configuration, writer, semaphore, cancellationToken);
                    tasks.Add(task);

                    // ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                    tasks.RemoveAll(t => t.IsCompleted);
                }

                // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                await Task.WhenAll(tasks);
                writer.Complete();
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Chunking pipeline failed");
                writer.Complete(ex);
            }
        }, cancellationToken);

        // ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        await foreach (var chunk in reader.ReadAllAsync(cancellationToken))
        {
            documentCount = Math.Max(documentCount, chunk.AdditionalMetadata.ContainsKey("DocumentNumber") ?
                (int)chunk.AdditionalMetadata["DocumentNumber"] : documentCount);
            totalChunks++;
            yield return chunk;
        }

        await chunkingTask;
        _logger.LogInformation("Chunked {Count} documents â†’ {TotalChunks} chunks",
            documentCount, totalChunks);
    }

    /// <summary>
    /// ê°œë³„ ì²­í‚¹ ì²˜ë¦¬ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
    /// </summary>
    private async Task ProcessSingleChunking(
        ExtractedContent extracted,
        int documentNumber,
        WebFluxConfiguration configuration,
        ChannelWriter<WebContentChunk> writer,
        SemaphoreSlim semaphore,
        CancellationToken cancellationToken)
    {
        await semaphore.WaitAsync(cancellationToken);

        try
        {
            _logger.LogInformation("  ğŸ“¦ Chunking document {DocNum}: Text={TextLength} chars, MainContent={MainLength} chars",
                documentNumber, extracted.Text?.Length ?? 0, extracted.MainContent?.Length ?? 0);

            var chunkingStrategy = _serviceFactory.CreateChunkingStrategy(configuration.Chunking.DefaultStrategy);

            // ChunkingConfigurationì„ ChunkingOptionsë¡œ ë³€í™˜
            var chunkingOptions = new ChunkingOptions
            {
                MaxChunkSize = configuration.Chunking.MaxChunkSize,
                ChunkOverlap = 50, // ê¸°ë³¸ê°’
                PreserveHeaders = true // ê¸°ë³¸ê°’
            };

            var chunks = await chunkingStrategy.ChunkAsync(
                extracted,
                chunkingOptions,
                cancellationToken);

            _logger.LogInformation("  âœ… Chunked document {DocNum} â†’ {ChunkCount} chunks",
                documentNumber, chunks.Count);

            // ê° ì²­í¬ì— document number ë©”íƒ€ë°ì´í„° ì¶”ê°€
            foreach (var chunk in chunks)
            {
                chunk.AdditionalMetadata["DocumentNumber"] = documentNumber;
                await writer.WriteAsync(chunk, cancellationToken);
            }
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to chunk document {DocNum}", documentNumber);
            // ì—ëŸ¬ê°€ ë°œìƒí•´ë„ íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰
        }
        finally
        {
            semaphore.Release();
        }
    }

    /// <summary>
    /// ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    /// </summary>
    public void Dispose()
    {
        _processingSlot?.Dispose();
    }

    private string ExtractTitle(string? content)
    {
        if (string.IsNullOrEmpty(content))
            return "Untitled";

        var titleMatch = System.Text.RegularExpressions.Regex.Match(content, @"<title[^>]*>([^<]+)</title>",
            System.Text.RegularExpressions.RegexOptions.IgnoreCase);

        return titleMatch.Success ? titleMatch.Groups[1].Value.Trim() : "Untitled";
    }

    // IWebContentProcessor interface implementations (stub implementations for build completion)

    public async Task<IReadOnlyList<WebContentChunk>> ProcessUrlAsync(
        string url,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        _logger.LogInformation("Processing single URL: {Url}", url);

        // WebFluxConfiguration ìƒì„±í•˜ì—¬ ë‹¨ì¼ URL ì²˜ë¦¬
        var configuration = new WebFluxConfiguration
        {
            Crawling = new CrawlingConfiguration
            {
                StartUrls = new List<string> { url },
                Strategy = "Dynamic", // Phase 1: Playwright ê¸°ë°˜ ë™ì  ë Œë”ë§ ì‚¬ìš©
                DefaultDelayMs = 0 // ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì œê±°
            },
            Chunking = new ChunkingConfiguration
            {
                DefaultStrategy = chunkingOptions?.Strategy.ToString() ?? "Auto",
                MaxChunkSize = chunkingOptions?.MaxChunkSize ?? 1000,
                MinChunkSize = chunkingOptions?.MinChunkSize ?? 100
            },
            AiEnhancement = new AiEnhancementConfiguration
            {
                // AI ì¦ê°•ì€ êµ¬ì„±ì—ì„œ ì„¤ì •ëœ ê°’ ì‚¬ìš© (ê¸°ë³¸ê°’: false)
                Enabled = true, // í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í™œì„±í™”
                EnableSummary = true,
                EnableMetadata = true
            }
        };

        // ProcessAsyncë¥¼ í˜¸ì¶œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        var chunks = new List<WebContentChunk>();

        await foreach (var chunk in ProcessAsync(configuration, cancellationToken))
        {
            chunks.Add(chunk);
        }

        _logger.LogInformation("Completed processing URL {Url}: {ChunkCount} chunks generated",
            url, chunks.Count);

        return chunks.AsReadOnly();
    }

    public async Task<IReadOnlyDictionary<string, IReadOnlyList<WebContentChunk>>> ProcessUrlsBatchAsync(
        IEnumerable<string> urls,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        // Stub implementation
        await Task.CompletedTask;
        return new Dictionary<string, IReadOnlyList<WebContentChunk>>().AsReadOnly();
    }

    public async IAsyncEnumerable<WebContentChunk> ProcessWebsiteAsync(
        string startUrl,
        CrawlOptions? crawlOptions = null,
        ChunkingOptions? chunkingOptions = null,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        _logger.LogInformation("Processing website: {Url} (Dynamic: {Dynamic})",
            startUrl, crawlOptions?.UseDynamicRendering ?? false);

        // WebFluxConfiguration ìƒì„±
        var configuration = new WebFluxConfiguration
        {
            Crawling = new CrawlingConfiguration
            {
                StartUrls = new List<string> { startUrl },
                // CrawlOptionsê°€ ì œê³µë˜ê³  UseDynamicRenderingì´ trueë©´ Dynamic ì „ëµ ì‚¬ìš©
                Strategy = (crawlOptions?.UseDynamicRendering == true || crawlOptions?.Strategy == WebFlux.Core.Options.CrawlStrategy.Dynamic)
                    ? "Dynamic"
                    : "BreadthFirst",
                DefaultDelayMs = crawlOptions?.DelayMs ?? 0
            },
            Chunking = new ChunkingConfiguration
            {
                DefaultStrategy = chunkingOptions?.Strategy.ToString() ?? "Auto",
                MaxChunkSize = chunkingOptions?.MaxChunkSize ?? 1000,
                MinChunkSize = chunkingOptions?.MinChunkSize ?? 100
            },
            AiEnhancement = new AiEnhancementConfiguration
            {
                Enabled = false
            },
            Performance = new PerformanceConfiguration
            {
                MaxDegreeOfParallelism = 3
            }
        };

        // CrawlOptionsë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§
        var crawlStrategy = (crawlOptions?.UseDynamicRendering == true || crawlOptions?.Strategy == WebFlux.Core.Options.CrawlStrategy.Dynamic)
            ? WebFlux.Core.Options.CrawlStrategy.Dynamic
            : WebFlux.Core.Options.CrawlStrategy.BreadthFirst;

        var crawler = _serviceFactory.CreateCrawler(crawlStrategy);

        // CrawlOptions ì ìš©
        var effectiveCrawlOptions = crawlOptions ?? new CrawlOptions
        {
            MaxDepth = 1,
            MaxPages = 1,
            DelayMs = 0,
            TimeoutMs = 15000
        };

        _logger.LogInformation("Using crawler: {Strategy}, UseDynamicRendering: {Dynamic}",
            crawlStrategy, effectiveCrawlOptions.UseDynamicRendering);

        // í¬ë¡¤ë§ ë° ì²˜ë¦¬
        await foreach (var crawlResult in crawler.CrawlWebsiteAsync(startUrl, effectiveCrawlOptions, cancellationToken))
        {
            if (string.IsNullOrEmpty(crawlResult.Content))
            {
                _logger.LogWarning("Empty content from {Url}", crawlResult.Url);
                continue;
            }

            // ì½˜í…ì¸  ì¶”ì¶œ
            var extractor = _serviceFactory.CreateContentExtractor(crawlResult.ContentType ?? "text/html");
            var extracted = await extractor.ExtractAutoAsync(
                crawlResult.Content,
                crawlResult.Url,
                crawlResult.ContentType ?? "text/html",
                cancellationToken);

            if (string.IsNullOrEmpty(extracted.MainContent) && string.IsNullOrEmpty(extracted.Text))
            {
                _logger.LogWarning("No extractable content from {Url}", crawlResult.Url);
                continue;
            }

            // ì²­í‚¹
            var chunkingStrategy = _serviceFactory.CreateChunkingStrategy(
                (chunkingOptions?.Strategy ?? ChunkingStrategyType.Auto).ToString());

            var effectiveChunkingOptions = chunkingOptions ?? new ChunkingOptions
            {
                MaxChunkSize = 1000,
                ChunkOverlap = 50,
                PreserveHeaders = true
            };

            var chunks = await chunkingStrategy.ChunkAsync(
                extracted,
                effectiveChunkingOptions,
                cancellationToken);

            _logger.LogInformation("Generated {ChunkCount} chunks from {Url}", chunks.Count, crawlResult.Url);

            foreach (var chunk in chunks)
            {
                yield return chunk;
            }
        }
    }

    public async Task<IReadOnlyList<WebContentChunk>> ProcessHtmlAsync(
        string htmlContent,
        string sourceUrl,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        // Stub implementation
        await Task.CompletedTask;
        return new List<WebContentChunk>().AsReadOnly();
    }

    public async IAsyncEnumerable<ProcessingProgress> MonitorProgressAsync(string jobId)
    {
        // Stub implementation
        await Task.CompletedTask;
        yield break;
    }

    public async Task<bool> CancelJobAsync(string jobId)
    {
        // Stub implementation
        await Task.CompletedTask;
        return true;
    }

    public async Task<ProcessingStatistics> GetStatisticsAsync()
    {
        // Stub implementation
        await Task.CompletedTask;
        return new ProcessingStatistics();
    }

    public IReadOnlyList<string> GetAvailableChunkingStrategies()
    {
        // Stub implementation
        return new List<string> { "FixedSize", "Paragraph", "Smart", "Semantic", "Auto", "MemoryOptimized" }.AsReadOnly();
    }

    #region ê²½ëŸ‰ ì¶”ì¶œ API êµ¬í˜„

    /// <summary>
    /// ë‹¨ì¼ URLì—ì„œ ì½˜í…ì¸ ë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤ (ì²­í‚¹ ì—†ìŒ)
    /// </summary>
    public async Task<ProcessingResult<ExtractedContent>> ExtractContentAsync(
        string url,
        ExtractOptions? options = null,
        CancellationToken cancellationToken = default)
    {
        var startTime = DateTimeOffset.UtcNow;
        var sw = System.Diagnostics.Stopwatch.StartNew();
        options ??= ExtractOptions.Default;

        try
        {
            _logger.LogDebug("Extracting content from {Url}", url);

            // URL ìœ íš¨ì„± ê²€ì‚¬
            if (!Uri.TryCreate(url, UriKind.Absolute, out var uri))
            {
                return ProcessingResult<ExtractedContent>.FromError(
                    "Invalid URL format",
                    ExtractErrorCodes.InvalidUrl,
                    sw.ElapsedMilliseconds);
            }

            // ìºì‹œ í™•ì¸ (UseCache && !ForceRefresh)
            var cacheService = _serviceFactory.TryCreateCacheService();
            if (options.UseCache && !options.ForceRefresh && cacheService != null)
            {
                var cacheKey = $"extract:{url}:{options.Format}";
                var cached = await cacheService.GetAsync<ExtractedContent>(cacheKey, cancellationToken).ConfigureAwait(false);

                if (cached != null)
                {
                    _logger.LogDebug("Cache hit for {Url}", url);
                    cached.FromCache = true;
                    return ProcessingResult<ExtractedContent>.Success(
                        cached,
                        processingTimeMs: sw.ElapsedMilliseconds,
                        metadata: new Dictionary<string, object> { ["cacheHit"] = true });
                }
            }

            // í¬ë¡¤ë§ ì „ëµ ì„ íƒ
            var crawlStrategy = options.UseDynamicRendering
                ? Core.Options.CrawlStrategy.Dynamic
                : Core.Options.CrawlStrategy.BreadthFirst;

            var crawler = _serviceFactory.CreateCrawler(crawlStrategy);

            var crawlOptions = new CrawlOptions
            {
                MaxDepth = 0,
                MaxPages = 1,
                TimeoutMs = options.TimeoutSeconds * 1000,
                UserAgent = options.UserAgent,
                WaitForSelector = options.WaitForSelector,
                UseDynamicRendering = options.UseDynamicRendering,
                CustomHeaders = options.CustomHeaders
            };

            // í¬ë¡¤ë§ ì‹¤í–‰ (ì¬ì‹œë„ ë¡œì§ í¬í•¨)
            CrawlResult? crawlResult = null;
            var retryCount = 0;

            while (retryCount <= options.MaxRetries)
            {
                try
                {
                    crawlResult = await crawler.CrawlAsync(url, crawlOptions, cancellationToken).ConfigureAwait(false);

                    if (crawlResult.IsSuccess)
                    {
                        break;
                    }
                }
                catch (Exception ex) when (retryCount < options.MaxRetries)
                {
                    _logger.LogWarning(ex, "Crawl attempt {Attempt} failed for {Url}", retryCount + 1, url);
                }

                retryCount++;
                if (retryCount <= options.MaxRetries)
                {
                    await Task.Delay(TimeSpan.FromSeconds(Math.Pow(2, retryCount - 1)), cancellationToken).ConfigureAwait(false);
                }
            }

            if (crawlResult == null || !crawlResult.IsSuccess)
            {
                var errorCode = crawlResult?.StatusCode != null
                    ? ExtractErrorCodes.FromHttpStatusCode(crawlResult.StatusCode)
                    : ExtractErrorCodes.NetworkError;

                return ProcessingResult<ExtractedContent>.FromError(
                    crawlResult?.ErrorMessage ?? "Failed to crawl URL",
                    errorCode,
                    sw.ElapsedMilliseconds);
            }

            if (string.IsNullOrWhiteSpace(crawlResult.Content))
            {
                return ProcessingResult<ExtractedContent>.FromError(
                    "Empty content received",
                    ExtractErrorCodes.EmptyContent,
                    sw.ElapsedMilliseconds);
            }

            // ì½˜í…ì¸  ì¶”ì¶œ
            var extractor = _serviceFactory.CreateContentExtractor(crawlResult.ContentType ?? "text/html");
            var extracted = await extractor.ExtractAutoAsync(
                crawlResult.Content,
                url,
                crawlResult.ContentType ?? "text/html",
                cancellationToken).ConfigureAwait(false);

            // í¬ë§· ë³€í™˜
            extracted = ApplyOutputFormat(extracted, options.Format, crawlResult.Content);

            // í…ìŠ¤íŠ¸ ê¸¸ì´ ì œí•œ ì ìš©
            if (options.MaxTextLength.HasValue && extracted.MainContent?.Length > options.MaxTextLength.Value)
            {
                extracted.MainContent = extracted.MainContent.Substring(0, options.MaxTextLength.Value);
            }

            // í’ˆì§ˆ í‰ê°€ (ì˜µì…˜ì´ í™œì„±í™”ëœ ê²½ìš°)
            if (options.EvaluateQuality)
            {
                var qualityEvaluator = _serviceFactory.TryCreateContentQualityEvaluator();
                if (qualityEvaluator != null)
                {
                    extracted.Quality = await qualityEvaluator.EvaluateAsync(
                        extracted,
                        crawlResult.Content,
                        cancellationToken).ConfigureAwait(false);
                }
            }

            // Boilerplate ì œê±° ì ìš©
            if (options.RemoveBoilerplate)
            {
                // ì´ë¯¸ ì¶”ì¶œê¸°ì—ì„œ MainContentë¡œ ë³¸ë¬¸ ì¶”ì¶œë¨
                // ì¶”ê°€ í´ë¦¬ë‹ì´ í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì„œ ì²˜ë¦¬
            }

            // ì´ë¯¸ì§€/ë§í¬ í¬í•¨ ì—¬ë¶€ ì²˜ë¦¬
            if (!options.IncludeImages)
            {
                extracted.ImageUrls = new List<string>();
            }

            // ë©”íƒ€ë°ì´í„° í¬í•¨ ì—¬ë¶€ ì²˜ë¦¬
            if (!options.IncludeMetadata)
            {
                extracted.Metadata = null;
            }

            extracted.ProcessingTimeMs = (int)sw.ElapsedMilliseconds;
            extracted.OriginalHtml = crawlResult.Content;

            // ìºì‹œ ì €ì¥
            if (options.UseCache && cacheService != null)
            {
                var cacheKey = $"extract:{url}:{options.Format}";
                await cacheService.SetAsync(
                    cacheKey,
                    extracted,
                    TimeSpan.FromMinutes(options.CacheExpirationMinutes),
                    cancellationToken).ConfigureAwait(false);
            }

            _logger.LogInformation(
                "Extracted content from {Url}: {CharCount} chars in {ElapsedMs}ms",
                url, extracted.MainContent?.Length ?? 0, sw.ElapsedMilliseconds);

            return ProcessingResult<ExtractedContent>.Success(
                extracted,
                processingTimeMs: sw.ElapsedMilliseconds);
        }
        catch (OperationCanceledException)
        {
            return ProcessingResult<ExtractedContent>.FromError(
                "Operation cancelled",
                ExtractErrorCodes.Timeout,
                sw.ElapsedMilliseconds);
        }
        catch (Exception ex)
        {
            _logger.LogError(ex, "Failed to extract content from {Url}", url);
            return ProcessingResult<ExtractedContent>.FromException(ex, sw.ElapsedMilliseconds);
        }
    }

    /// <summary>
    /// ì—¬ëŸ¬ URLì—ì„œ ì½˜í…ì¸ ë¥¼ ë°°ì¹˜ ì¶”ì¶œí•©ë‹ˆë‹¤
    /// </summary>
    public async Task<BatchExtractResult> ExtractBatchAsync(
        IEnumerable<string> urls,
        ExtractOptions? options = null,
        CancellationToken cancellationToken = default)
    {
        var startTime = DateTimeOffset.UtcNow;
        var sw = System.Diagnostics.Stopwatch.StartNew();
        options ??= ExtractOptions.Default;

        var urlList = urls.ToList();
        var succeeded = new ConcurrentBag<ExtractedContent>();
        var failed = new ConcurrentBag<FailedExtraction>();
        var cacheHits = 0;
        var processingTimes = new ConcurrentBag<long>();
        var domainCounts = new ConcurrentDictionary<string, int>();

        _logger.LogInformation("Starting batch extraction for {UrlCount} URLs", urlList.Count);

        // Rate Limiter ìƒì„±
        var rateLimiter = options.EnableDomainRateLimiting
            ? _serviceFactory.TryCreateDomainRateLimiter()
            : null;

        // ë³‘ë ¬ ì²˜ë¦¬
        var semaphore = new SemaphoreSlim(options.MaxConcurrency);
        var tasks = urlList.Select(async url =>
        {
            await semaphore.WaitAsync(cancellationToken).ConfigureAwait(false);

            try
            {
                var domain = GetDomain(url);
                domainCounts.AddOrUpdate(domain, 1, (_, count) => count + 1);

                ProcessingResult<ExtractedContent> result;

                if (rateLimiter != null && options.EnableDomainRateLimiting)
                {
                    result = await rateLimiter.ExecuteAsync(
                        domain,
                        () => ExtractContentAsync(url, options, cancellationToken),
                        cancellationToken).ConfigureAwait(false);
                }
                else
                {
                    result = await ExtractContentAsync(url, options, cancellationToken).ConfigureAwait(false);
                }

                processingTimes.Add(result.ProcessingTimeMs);

                if (result.IsSuccess && result.Data != null)
                {
                    succeeded.Add(result.Data);

                    if (result.Metadata.TryGetValue("cacheHit", out var hit) && (bool)hit)
                    {
                        Interlocked.Increment(ref cacheHits);
                    }
                }
                else
                {
                    failed.Add(new FailedExtraction
                    {
                        Url = url,
                        ErrorCode = result.Error?.Code ?? ExtractErrorCodes.Unknown,
                        ErrorMessage = result.Error?.Message ?? "Unknown error",
                        RetryCount = options.MaxRetries,
                        ProcessingTimeMs = result.ProcessingTimeMs,
                        Exception = result.Error?.InnerException
                    });
                }
            }
            catch (Exception ex)
            {
                failed.Add(new FailedExtraction
                {
                    Url = url,
                    ErrorCode = ExtractErrorCodes.Unknown,
                    ErrorMessage = ex.Message,
                    Exception = ex
                });
            }
            finally
            {
                semaphore.Release();
            }
        });

        await Task.WhenAll(tasks).ConfigureAwait(false);

        sw.Stop();

        // í†µê³„ ê³„ì‚°
        var succeededList = succeeded.ToList();
        var failedList = failed.ToList();
        var processingTimesList = processingTimes.ToList();

        var statistics = new BatchStatistics
        {
            AverageProcessingTimeMs = processingTimesList.Count > 0 ? processingTimesList.Average() : 0,
            TotalCharactersExtracted = succeededList.Sum(c => c.MainContent?.Length ?? 0),
            CacheHitRate = urlList.Count > 0 ? (double)cacheHits / urlList.Count : 0,
            CacheHits = cacheHits,
            CacheMisses = urlList.Count - cacheHits,
            ProcessedByDomain = domainCounts.ToDictionary(kvp => kvp.Key, kvp => kvp.Value),
            FailuresByErrorCode = failedList
                .GroupBy(f => f.ErrorCode)
                .ToDictionary(g => g.Key, g => g.Count()),
            MinProcessingTimeMs = processingTimesList.Count > 0 ? processingTimesList.Min() : 0,
            MaxProcessingTimeMs = processingTimesList.Count > 0 ? processingTimesList.Max() : 0,
            DynamicRenderingCount = options.UseDynamicRendering ? succeededList.Count : 0,
            StaticRenderingCount = options.UseDynamicRendering ? 0 : succeededList.Count
        };

        var result = new BatchExtractResult
        {
            Succeeded = succeededList,
            Failed = failedList,
            TotalDurationMs = sw.ElapsedMilliseconds,
            Statistics = statistics,
            StartTime = startTime,
            EndTime = DateTimeOffset.UtcNow
        };

        _logger.LogInformation(
            "Batch extraction completed: {Succeeded}/{Total} succeeded ({SuccessRate:P0}) in {Duration}ms",
            succeededList.Count, urlList.Count, result.SuccessRate, sw.ElapsedMilliseconds);

        return result;
    }

    /// <summary>
    /// ì—¬ëŸ¬ URLì—ì„œ ì½˜í…ì¸ ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°°ì¹˜ ì¶”ì¶œí•©ë‹ˆë‹¤
    /// </summary>
    public async IAsyncEnumerable<ProcessingResult<ExtractedContent>> ExtractBatchStreamAsync(
        IEnumerable<string> urls,
        ExtractOptions? options = null,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        options ??= ExtractOptions.Default;
        var urlList = urls.ToList();

        _logger.LogInformation("Starting streaming batch extraction for {UrlCount} URLs", urlList.Count);

        // Rate Limiter ìƒì„±
        var rateLimiter = options.EnableDomainRateLimiting
            ? _serviceFactory.TryCreateDomainRateLimiter()
            : null;

        // ì±„ë„ ê¸°ë°˜ ìŠ¤íŠ¸ë¦¬ë°
        var channel = Channel.CreateUnbounded<ProcessingResult<ExtractedContent>>();
        var writer = channel.Writer;
        var reader = channel.Reader;

        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¶”ì¶œ ì‘ì—… ì‹¤í–‰
        var extractionTask = Task.Run(async () =>
        {
            var semaphore = new SemaphoreSlim(options.MaxConcurrency);
            var tasks = urlList.Select(async url =>
            {
                await semaphore.WaitAsync(cancellationToken).ConfigureAwait(false);

                try
                {
                    ProcessingResult<ExtractedContent> result;

                    if (rateLimiter != null && options.EnableDomainRateLimiting)
                    {
                        var domain = GetDomain(url);
                        result = await rateLimiter.ExecuteAsync(
                            domain,
                            () => ExtractContentAsync(url, options, cancellationToken),
                            cancellationToken).ConfigureAwait(false);
                    }
                    else
                    {
                        result = await ExtractContentAsync(url, options, cancellationToken).ConfigureAwait(false);
                    }

                    await writer.WriteAsync(result, cancellationToken).ConfigureAwait(false);
                }
                catch (Exception ex)
                {
                    var failedResult = ProcessingResult<ExtractedContent>.FromException(ex);
                    await writer.WriteAsync(failedResult, cancellationToken).ConfigureAwait(false);
                }
                finally
                {
                    semaphore.Release();
                }
            });

            await Task.WhenAll(tasks).ConfigureAwait(false);
            writer.Complete();
        }, cancellationToken);

        // ê²°ê³¼ ìŠ¤íŠ¸ë¦¬ë°
        await foreach (var result in reader.ReadAllAsync(cancellationToken).ConfigureAwait(false))
        {
            yield return result;
        }

        await extractionTask.ConfigureAwait(false);
    }

    /// <summary>
    /// ì¶œë ¥ í¬ë§·ì„ ì ìš©í•©ë‹ˆë‹¤
    /// </summary>
    private ExtractedContent ApplyOutputFormat(ExtractedContent content, OutputFormat format, string originalHtml)
    {
        switch (format)
        {
            case OutputFormat.Markdown:
                // ì´ë¯¸ Markdown í˜•íƒœì¸ ê²½ìš° ê·¸ëŒ€ë¡œ ì‚¬ìš©
                // HTMLì—ì„œ Markdownìœ¼ë¡œ ë³€í™˜ì´ í•„ìš”í•œ ê²½ìš° ì—¬ê¸°ì„œ ì²˜ë¦¬
                // MainContentëŠ” ì´ë¯¸ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶”ì¶œë¨
                break;

            case OutputFormat.Html:
                // HTML ì›ë³¸ ìœ ì§€
                content.MainContent = originalHtml;
                break;

            case OutputFormat.PlainText:
                // ì´ë¯¸ í…ìŠ¤íŠ¸ í˜•íƒœë¡œ ì¶”ì¶œë¨
                // ì¶”ê°€ í´ë¦¬ë‹ (ë§ˆí¬ë‹¤ìš´ ë§ˆì»¤ ì œê±° ë“±)
                if (!string.IsNullOrEmpty(content.MainContent))
                {
                    content.MainContent = CleanPlainText(content.MainContent);
                }
                break;
        }

        return content;
    }

    /// <summary>
    /// í…ìŠ¤íŠ¸ë¥¼ í”Œë ˆì¸ í…ìŠ¤íŠ¸ë¡œ ì •ë¦¬í•©ë‹ˆë‹¤
    /// </summary>
    private static string CleanPlainText(string text)
    {
        // Markdown ë§ˆì»¤ ì œê±°
        text = System.Text.RegularExpressions.Regex.Replace(text, @"#{1,6}\s*", "");
        text = System.Text.RegularExpressions.Regex.Replace(text, @"\*\*([^*]+)\*\*", "$1");
        text = System.Text.RegularExpressions.Regex.Replace(text, @"\*([^*]+)\*", "$1");
        text = System.Text.RegularExpressions.Regex.Replace(text, @"\[([^\]]+)\]\([^)]+\)", "$1");
        text = System.Text.RegularExpressions.Regex.Replace(text, @"`([^`]+)`", "$1");

        return text.Trim();
    }

    /// <summary>
    /// URLì—ì„œ ë„ë©”ì¸ì„ ì¶”ì¶œí•©ë‹ˆë‹¤
    /// </summary>
    private static string GetDomain(string url)
    {
        if (Uri.TryCreate(url, UriKind.Absolute, out var uri))
        {
            return uri.Host.ToLowerInvariant();
        }

        return url.ToLowerInvariant();
    }

    #endregion
}

/// <summary>
/// ì²˜ë¦¬ ì‹œì‘ ì´ë²¤íŠ¸
/// </summary>
public class ProcessingStartedEvent : ProcessingEvent
{
    public override string EventType => "ProcessingStarted";
    public WebFluxConfiguration Configuration { get; set; } = new();
    public List<string> StartUrls { get; set; } = new();
}

/// <summary>
/// ì²˜ë¦¬ ì§„í–‰ë¥  ì´ë²¤íŠ¸
/// </summary>
public class ProcessingProgressEvent : ProcessingEvent
{
    public override string EventType => "ProcessingProgress";
    public int ProcessedCount { get; set; }
    public TimeSpan ElapsedTime { get; set; }
    public TimeSpan EstimatedRemaining { get; set; }
    public string CurrentStage { get; set; } = string.Empty;
}

/// <summary>
/// ì²˜ë¦¬ ì™„ë£Œ ì´ë²¤íŠ¸
/// </summary>
public class ProcessingCompletedEvent : ProcessingEvent
{
    public override string EventType => "ProcessingCompleted";
    public int ProcessedChunkCount { get; set; }
    public TimeSpan TotalProcessingTime { get; set; }
    public double AverageProcessingRate { get; set; }
}

/// <summary>
/// ì²˜ë¦¬ ì‹¤íŒ¨ ì´ë²¤íŠ¸
/// </summary>
public class ProcessingFailedEvent : ProcessingEvent
{
    public override string EventType => "ProcessingFailed";
    public string Error { get; set; } = string.Empty;
    public int ProcessedCount { get; set; }
}