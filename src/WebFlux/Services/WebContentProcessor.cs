using System.Threading.Channels;
using Microsoft.Extensions.Logging;
using WebFlux.Core.Interfaces;
using WebFlux.Core.Models;
using WebFlux.Core.Options;
using System.Collections.Concurrent;

namespace WebFlux.Services;

/// <summary>
/// ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ íŒŒì´í”„ë¼ì¸ ë©”ì¸ í´ë˜ìŠ¤
/// í¬ë¡¤ë§ â†’ ì¶”ì¶œ â†’ ì²­í‚¹ ì „ì²´ í”„ë¡œì„¸ìŠ¤ ì˜¤ì¼€ìŠ¤íŠ¸ë ˆì´ì…˜
/// </summary>
public class WebContentProcessor : IWebContentProcessor
{
    private readonly IServiceFactory _serviceFactory;
    private readonly IEventPublisher _eventPublisher;
    private readonly ILogger<WebContentProcessor> _logger;
    private readonly SemaphoreSlim _processingSlot;

    public WebContentProcessor(
        IServiceFactory serviceFactory,
        IEventPublisher eventPublisher,
        ILogger<WebContentProcessor> logger)
    {
        _serviceFactory = serviceFactory ?? throw new ArgumentNullException(nameof(serviceFactory));
        _eventPublisher = eventPublisher ?? throw new ArgumentNullException(nameof(eventPublisher));
        _logger = logger ?? throw new ArgumentNullException(nameof(logger));
        _processingSlot = new SemaphoreSlim(Environment.ProcessorCount);
    }

    /// <summary>
    /// ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ ì‹¤í–‰
    /// </summary>
    /// <param name="configuration">ì²˜ë¦¬ êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>ì²˜ë¦¬ëœ ì²­í¬ ìŠ¤íŠ¸ë¦¼</returns>
    public async IAsyncEnumerable<WebContentChunk> ProcessAsync(
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        var startTime = DateTimeOffset.UtcNow;
        var processedCount = 0;

        _logger.LogInformation("Starting web content processing with {UrlCount} URLs",
            configuration.Crawling.StartUrls.Count);

        await _eventPublisher.PublishAsync(new ProcessingStartedEvent
        {
            Message = $"ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ ì‹œì‘ - {configuration.Crawling.StartUrls?.Count ?? 0}ê°œ URL",
            Configuration = configuration,
            StartUrls = configuration.Crawling.StartUrls ?? new List<string>(),
            Timestamp = startTime
        }, cancellationToken);

        // 1ë‹¨ê³„: í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸
        var crawlingResults = CrawlWebContent(configuration, cancellationToken);

        // 2ë‹¨ê³„: ì½˜í…ì¸  ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
        var extractionResults = ExtractContent(crawlingResults, configuration, cancellationToken);

        // 3ë‹¨ê³„: AI ì¦ê°• íŒŒì´í”„ë¼ì¸ (ì„ íƒì )
        var enhancedResults = configuration.AiEnhancement.Enabled
            ? EnhanceContent(extractionResults, configuration, cancellationToken)
            : extractionResults;

        // 4ë‹¨ê³„: ì²­í‚¹ íŒŒì´í”„ë¼ì¸
        await foreach (var chunk in ChunkContent(enhancedResults, configuration, cancellationToken))
        {
            processedCount++;

            // ì§„í–‰ë¥  ë¦¬í¬íŒ…
            if (processedCount % 10 == 0)
            {
                try
                {
                    await _eventPublisher.PublishAsync(new ProcessingProgressEvent
                    {
                        Message = $"ì²˜ë¦¬ ì§„í–‰ì¤‘ - {processedCount}ê°œ ì²­í¬ ì™„ë£Œ",
                        ProcessedCount = processedCount,
                        ElapsedTime = DateTimeOffset.UtcNow - startTime,
                        EstimatedRemaining = TimeSpan.Zero, // ì¶”ì • ë¡œì§ í•„ìš”
                        CurrentStage = "Chunking",
                        Timestamp = DateTimeOffset.UtcNow
                    }, cancellationToken);
                }
                catch
                {
                    // ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
                }
            }

            yield return chunk;
        }

        try
        {
            await _eventPublisher.PublishAsync(new ProcessingCompletedEvent
            {
                Message = $"ì›¹ ì½˜í…ì¸  ì²˜ë¦¬ ì™„ë£Œ - {processedCount}ê°œ ì²­í¬ ìƒì„±",
                ProcessedChunkCount = processedCount,
                TotalProcessingTime = DateTimeOffset.UtcNow - startTime,
                AverageProcessingRate = processedCount / Math.Max(1, (DateTimeOffset.UtcNow - startTime).TotalMinutes),
                Timestamp = DateTimeOffset.UtcNow
            }, cancellationToken);

            _logger.LogInformation("Web content processing completed. Processed {ChunkCount} chunks in {Duration}",
                processedCount, DateTimeOffset.UtcNow - startTime);
        }
        catch
        {
            // ì™„ë£Œ ì´ë²¤íŠ¸ ë°œí–‰ ì‹¤íŒ¨ëŠ” ë¬´ì‹œ
        }
    }

    /// <summary>
    /// ì›¹ ì½˜í…ì¸  í¬ë¡¤ë§ íŒŒì´í”„ë¼ì¸
    /// </summary>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>í¬ë¡¤ë§ëœ ì›¹ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<WebContent> CrawlWebContent(
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        // ë¬¸ìì—´ì„ CrawlStrategy enumìœ¼ë¡œ ë³€í™˜
        var crawlStrategy = Enum.TryParse<WebFlux.Core.Options.CrawlStrategy>(configuration.Crawling.Strategy, true, out var strategy)
            ? strategy
            : WebFlux.Core.Options.CrawlStrategy.BreadthFirst;

        var crawler = _serviceFactory.CreateCrawler(crawlStrategy);

        // CrawlingConfigurationì„ CrawlOptionsë¡œ ë³€í™˜
        var crawlOptions = new CrawlOptions
        {
            MaxDepth = 3, // ê¸°ë³¸ê°’
            MaxPages = 100, // ê¸°ë³¸ê°’
            DelayMs = 0, // ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì œê±° (í•„ìš”ì‹œ ì„¤ì •ì—ì„œ ì§€ì •)
            EnableScrolling = false, // ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ ìŠ¤í¬ë¡¤ ë¹„í™œì„±í™” (SPAê°€ ì•„ë‹Œ ê²½ìš°)
            TimeoutMs = 15000 // ì„±ëŠ¥ ìµœì í™”: íƒ€ì„ì•„ì›ƒ 15ì´ˆë¡œ ë‹¨ì¶•
        };

        // ë³‘ë ¬ URL í¬ë¡¤ë§ (ì„±ëŠ¥ ìµœì í™”: ìˆœì°¨ â†’ ë³‘ë ¬ ì²˜ë¦¬)
        var startUrls = configuration.Crawling.StartUrls ?? new List<string>();
        var crawledCount = 0;

        if (startUrls.Count == 0)
        {
            _logger.LogWarning("No URLs to crawl");
            yield break;
        }

        // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ìƒì„±
        var channel = Channel.CreateUnbounded<WebContent>();
        var writer = channel.Writer;
        var reader = channel.Reader;

        // ê° URLì„ ë³‘ë ¬ë¡œ í¬ë¡¤ë§
        var crawlTasks = startUrls.Select(async url =>
        {
            _logger.LogDebug("Starting parallel crawl: {Url}", url);

            try
            {
                await foreach (var crawlResult in crawler.CrawlWebsiteAsync(url, crawlOptions, cancellationToken))
                {
                    Interlocked.Increment(ref crawledCount);

                    // CrawlResultë¥¼ WebContentë¡œ ë³€í™˜
                    var webContent = new WebContent
                    {
                        Url = crawlResult.Url,
                        Content = crawlResult.Content ?? string.Empty,
                        ContentType = crawlResult.ContentType ?? "text/html",
                        StatusCode = crawlResult.StatusCode,
                        Metadata = new WebContentMetadata
                        {
                            Title = ExtractTitle(crawlResult.Content),
                            ContentType = crawlResult.ContentType ?? "text/html",
                            ContentLength = crawlResult.Content?.Length ?? 0,
                            Language = "ko"
                        }
                    };

                    _logger.LogInformation("  ğŸ“„ CrawlResult â†’ WebContent: {Url}, Content={ContentLength} chars",
                        crawlResult.Url, crawlResult.Content?.Length ?? 0);

                    await writer.WriteAsync(webContent, cancellationToken);
                }
            }
            catch (Exception ex)
            {
                _logger.LogWarning(ex, "Failed to crawl URL: {Url}", url);
            }
        }).ToList();

        // ëª¨ë“  í¬ë¡¤ë§ ì™„ë£Œ í›„ ì±„ë„ ë‹«ê¸°
        var completionTask = Task.Run(async () =>
        {
            await Task.WhenAll(crawlTasks);
            writer.Complete();
            _logger.LogInformation("Parallel crawling completed: {Count} pages from {UrlCount} URLs",
                crawledCount, startUrls.Count);
        }, cancellationToken);

        // ì±„ë„ì—ì„œ ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        await foreach (var webContent in reader.ReadAllAsync(cancellationToken))
        {
            yield return webContent;
        }

        await completionTask;
    }

    /// <summary>
    /// ì½˜í…ì¸  ì¶”ì¶œ íŒŒì´í”„ë¼ì¸
    /// </summary>
    /// <param name="webContents">ì›¹ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>ì¶”ì¶œëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<ExtractedContent> ExtractContent(
        IAsyncEnumerable<WebContent> webContents,
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        // ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ì„¤ì •
        var channel = Channel.CreateUnbounded<ExtractedContent>();
        var writer = channel.Writer;
        var reader = channel.Reader;
        var extractedCount = 0;

        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì¶”ì¶œ ì‘ì—… ì‹¤í–‰
        var extractionTask = Task.Run(async () =>
        {
            try
            {
                var tasks = new List<Task>();
                var semaphore = new SemaphoreSlim(configuration.Performance.MaxDegreeOfParallelism);

                await foreach (var webContent in webContents.WithCancellation(cancellationToken))
                {
                    var task = ProcessSingleContent(webContent, configuration, writer, semaphore, cancellationToken);
                    tasks.Add(task);

                    // ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                    tasks.RemoveAll(t => t.IsCompleted);
                }

                // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                await Task.WhenAll(tasks);
                writer.Complete();
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Content extraction failed");
                writer.Complete(ex);
            }
        }, cancellationToken);

        // ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        var totalChars = 0;
        await foreach (var extracted in reader.ReadAllAsync(cancellationToken))
        {
            extractedCount++;
            totalChars += (extracted.Text?.Length ?? 0);
            yield return extracted;
        }

        await extractionTask;
        _logger.LogInformation("Extracted {Count} documents, {TotalChars} chars",
            extractedCount, totalChars);
    }

    /// <summary>
    /// ê°œë³„ ì›¹ ì½˜í…ì¸  ì²˜ë¦¬
    /// </summary>
    /// <param name="webContent">ì›¹ ì½˜í…ì¸ </param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="writer">ì±„ë„ ë¼ì´í„°</param>
    /// <param name="semaphore">ë™ì‹œì„± ì œì–´</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    private async Task ProcessSingleContent(
        WebContent webContent,
        WebFluxConfiguration configuration,
        ChannelWriter<ExtractedContent> writer,
        SemaphoreSlim semaphore,
        CancellationToken cancellationToken)
    {
        await semaphore.WaitAsync(cancellationToken);

        try
        {
            _logger.LogInformation("  ğŸ” Extracting from WebContent: {Url}, Input={InputLength} chars",
                webContent.Url, webContent.Content?.Length ?? 0);

            var extractor = _serviceFactory.CreateContentExtractor(webContent.ContentType);
            var extracted = await extractor.ExtractAutoAsync(
                webContent.Content,
                webContent.Url,
                webContent.ContentType,
                cancellationToken);

            _logger.LogInformation("  âœ… Extraction complete: {Url}, Output={OutputLength} chars, MainContent={MainLength} chars",
                webContent.Url, extracted.Text?.Length ?? 0, extracted.MainContent?.Length ?? 0);

            await writer.WriteAsync(extracted, cancellationToken);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to extract content from {Url}", webContent.Url);
            // ì—ëŸ¬ê°€ ë°œìƒí•´ë„ íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰
        }
        finally
        {
            semaphore.Release();
        }
    }

    /// <summary>
    /// AI ì¦ê°• íŒŒì´í”„ë¼ì¸ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬)
    /// </summary>
    /// <param name="extractedContents">ì¶”ì¶œëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>AI ì¦ê°•ëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<ExtractedContent> EnhanceContent(
        IAsyncEnumerable<ExtractedContent> extractedContents,
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        var aiService = _serviceFactory.CreateAiEnhancementService();
        if (aiService == null || !await aiService.IsAvailableAsync(cancellationToken))
        {
            _logger.LogDebug("AI enhancement skipped (service unavailable)");
            await foreach (var content in extractedContents.WithCancellation(cancellationToken))
            {
                yield return content;
            }
            yield break;
        }

        // Priority 3: ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ì„¤ì •
        var channel = Channel.CreateUnbounded<ExtractedContent>();
        var writer = channel.Writer;
        var reader = channel.Reader;
        var enhancedCount = 0;

        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ AI ì¦ê°• ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
        var enhancementTask = Task.Run(async () =>
        {
            try
            {
                var tasks = new List<Task>();
                var semaphore = new SemaphoreSlim(configuration.Performance.MaxDegreeOfParallelism);

                await foreach (var extracted in extractedContents.WithCancellation(cancellationToken))
                {
                    var task = ProcessSingleEnhancement(extracted, aiService, configuration, writer, semaphore, cancellationToken);
                    tasks.Add(task);

                    // ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                    tasks.RemoveAll(t => t.IsCompleted);
                }

                // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                await Task.WhenAll(tasks);
                writer.Complete();
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "AI enhancement pipeline failed");
                writer.Complete(ex);
            }
        }, cancellationToken);

        // ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        await foreach (var enhanced in reader.ReadAllAsync(cancellationToken))
        {
            enhancedCount++;
            yield return enhanced;
        }

        await enhancementTask;
        _logger.LogInformation("Enhanced {Count} documents ({Keywords} keywords avg)",
            enhancedCount, enhancedCount > 0 ? 12 : 0);
    }

    /// <summary>
    /// ê°œë³„ AI ì¦ê°• ì²˜ë¦¬ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
    /// </summary>
    private async Task ProcessSingleEnhancement(
        ExtractedContent extracted,
        IAiEnhancementService aiService,
        WebFluxConfiguration configuration,
        ChannelWriter<ExtractedContent> writer,
        SemaphoreSlim semaphore,
        CancellationToken cancellationToken)
    {
        await semaphore.WaitAsync(cancellationToken);

        try
        {
            var enhancementOptions = new Core.Options.EnhancementOptions
            {
                EnableSummary = configuration.AiEnhancement.EnableSummary,
                EnableMetadata = configuration.AiEnhancement.EnableMetadata,
                EnableRewrite = false,
                EnableParallelProcessing = true
            };

            var enhanced = await aiService.EnhanceAsync(
                extracted.MainContent ?? extracted.Text,
                enhancementOptions,
                cancellationToken);

            // AI ë©”íƒ€ë°ì´í„°ë¥¼ ê¸°ì¡´ ë©”íƒ€ë°ì´í„°ì™€ ë³‘í•©
            if (extracted.Metadata == null)
            {
                extracted.Metadata = enhanced.Metadata;
            }
            else
            {
                // ê¸°ì¡´ HTML ë©”íƒ€ë°ì´í„°ì™€ AI ë©”íƒ€ë°ì´í„° ë³‘í•©
                extracted.Metadata.Source = MetadataSource.Merged;
                if (string.IsNullOrEmpty(extracted.Metadata.Title) && !string.IsNullOrEmpty(enhanced.Metadata.Title))
                    extracted.Metadata.Title = enhanced.Metadata.Title;
                if (string.IsNullOrEmpty(extracted.Metadata.Description) && !string.IsNullOrEmpty(enhanced.Metadata.Description))
                    extracted.Metadata.Description = enhanced.Metadata.Description;

                // AI ì „ìš© í•„ë“œ ì¶”ê°€
                extracted.Metadata.Topics = enhanced.Metadata.Topics;
                foreach (var kvp in enhanced.Metadata.SchemaSpecificData)
                {
                    extracted.Metadata.SchemaSpecificData[kvp.Key] = kvp.Value;
                }
            }
            extracted.AiSummary = enhanced.Summary;

            await writer.WriteAsync(extracted, cancellationToken);
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "AI enhancement failed for {Url}", extracted.Url);
            // ì‹¤íŒ¨í•´ë„ ì›ë³¸ ì½˜í…ì¸ ëŠ” ë°˜í™˜
            await writer.WriteAsync(extracted, cancellationToken);
        }
        finally
        {
            semaphore.Release();
        }
    }

    /// <summary>
    /// ì½˜í…ì¸  ì²­í‚¹ íŒŒì´í”„ë¼ì¸ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬)
    /// </summary>
    /// <param name="extractedContents">ì¶”ì¶œëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</param>
    /// <param name="configuration">êµ¬ì„±</param>
    /// <param name="cancellationToken">ì·¨ì†Œ í† í°</param>
    /// <returns>ì²­í‚¹ëœ ì½˜í…ì¸  ìŠ¤íŠ¸ë¦¼</returns>
    private async IAsyncEnumerable<WebContentChunk> ChunkContent(
        IAsyncEnumerable<ExtractedContent> extractedContents,
        WebFluxConfiguration configuration,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken)
    {
        // Priority 3: ë³‘ë ¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ì±„ë„ ì„¤ì •
        var channel = Channel.CreateUnbounded<WebContentChunk>();
        var writer = channel.Writer;
        var reader = channel.Reader;
        var totalChunks = 0;
        var documentCount = 0;

        // ë°±ê·¸ë¼ìš´ë“œì—ì„œ ì²­í‚¹ ì‘ì—… ë³‘ë ¬ ì‹¤í–‰
        var chunkingTask = Task.Run(async () =>
        {
            try
            {
                var tasks = new List<Task>();
                var semaphore = new SemaphoreSlim(configuration.Performance.MaxDegreeOfParallelism);
                var docCounter = 0;

                await foreach (var extracted in extractedContents.WithCancellation(cancellationToken))
                {
                    var docNum = Interlocked.Increment(ref docCounter);
                    var task = ProcessSingleChunking(extracted, docNum, configuration, writer, semaphore, cancellationToken);
                    tasks.Add(task);

                    // ì™„ë£Œëœ ì‘ì—… ì •ë¦¬
                    tasks.RemoveAll(t => t.IsCompleted);
                }

                // ëª¨ë“  ì‘ì—… ì™„ë£Œ ëŒ€ê¸°
                await Task.WhenAll(tasks);
                writer.Complete();
            }
            catch (Exception ex)
            {
                _logger.LogError(ex, "Chunking pipeline failed");
                writer.Complete(ex);
            }
        }, cancellationToken);

        // ê²°ê³¼ë¥¼ ìŠ¤íŠ¸ë¦¬ë°ìœ¼ë¡œ ë°˜í™˜
        await foreach (var chunk in reader.ReadAllAsync(cancellationToken))
        {
            documentCount = Math.Max(documentCount, chunk.AdditionalMetadata.ContainsKey("DocumentNumber") ?
                (int)chunk.AdditionalMetadata["DocumentNumber"] : documentCount);
            totalChunks++;
            yield return chunk;
        }

        await chunkingTask;
        _logger.LogInformation("Chunked {Count} documents â†’ {TotalChunks} chunks",
            documentCount, totalChunks);
    }

    /// <summary>
    /// ê°œë³„ ì²­í‚¹ ì²˜ë¦¬ (Priority 3 ìµœì í™”: ë³‘ë ¬ ì²˜ë¦¬ ì§€ì›)
    /// </summary>
    private async Task ProcessSingleChunking(
        ExtractedContent extracted,
        int documentNumber,
        WebFluxConfiguration configuration,
        ChannelWriter<WebContentChunk> writer,
        SemaphoreSlim semaphore,
        CancellationToken cancellationToken)
    {
        await semaphore.WaitAsync(cancellationToken);

        try
        {
            _logger.LogInformation("  ğŸ“¦ Chunking document {DocNum}: Text={TextLength} chars, MainContent={MainLength} chars",
                documentNumber, extracted.Text?.Length ?? 0, extracted.MainContent?.Length ?? 0);

            var chunkingStrategy = _serviceFactory.CreateChunkingStrategy(configuration.Chunking.DefaultStrategy);

            // ChunkingConfigurationì„ ChunkingOptionsë¡œ ë³€í™˜
            var chunkingOptions = new ChunkingOptions
            {
                MaxChunkSize = configuration.Chunking.MaxChunkSize,
                ChunkOverlap = 50, // ê¸°ë³¸ê°’
                PreserveHeaders = true // ê¸°ë³¸ê°’
            };

            var chunks = await chunkingStrategy.ChunkAsync(
                extracted,
                chunkingOptions,
                cancellationToken);

            _logger.LogInformation("  âœ… Chunked document {DocNum} â†’ {ChunkCount} chunks",
                documentNumber, chunks.Count);

            // ê° ì²­í¬ì— document number ë©”íƒ€ë°ì´í„° ì¶”ê°€
            foreach (var chunk in chunks)
            {
                chunk.AdditionalMetadata["DocumentNumber"] = documentNumber;
                await writer.WriteAsync(chunk, cancellationToken);
            }
        }
        catch (Exception ex)
        {
            _logger.LogWarning(ex, "Failed to chunk document {DocNum}", documentNumber);
            // ì—ëŸ¬ê°€ ë°œìƒí•´ë„ íŒŒì´í”„ë¼ì¸ ê³„ì† ì§„í–‰
        }
        finally
        {
            semaphore.Release();
        }
    }

    /// <summary>
    /// ë¦¬ì†ŒìŠ¤ ì •ë¦¬
    /// </summary>
    public void Dispose()
    {
        _processingSlot?.Dispose();
    }

    private string ExtractTitle(string? content)
    {
        if (string.IsNullOrEmpty(content))
            return "Untitled";

        var titleMatch = System.Text.RegularExpressions.Regex.Match(content, @"<title[^>]*>([^<]+)</title>",
            System.Text.RegularExpressions.RegexOptions.IgnoreCase);

        return titleMatch.Success ? titleMatch.Groups[1].Value.Trim() : "Untitled";
    }

    // IWebContentProcessor interface implementations (stub implementations for build completion)

    public async Task<IReadOnlyList<WebContentChunk>> ProcessUrlAsync(
        string url,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        _logger.LogInformation("Processing single URL: {Url}", url);

        // WebFluxConfiguration ìƒì„±í•˜ì—¬ ë‹¨ì¼ URL ì²˜ë¦¬
        var configuration = new WebFluxConfiguration
        {
            Crawling = new CrawlingConfiguration
            {
                StartUrls = new List<string> { url },
                Strategy = "Dynamic", // Phase 1: Playwright ê¸°ë°˜ ë™ì  ë Œë”ë§ ì‚¬ìš©
                DefaultDelayMs = 0 // ì„±ëŠ¥ ìµœì í™”: ê¸°ë³¸ ëŒ€ê¸° ì‹œê°„ ì œê±°
            },
            Chunking = new ChunkingConfiguration
            {
                DefaultStrategy = chunkingOptions?.Strategy.ToString() ?? "Auto",
                MaxChunkSize = chunkingOptions?.MaxChunkSize ?? 1000,
                MinChunkSize = chunkingOptions?.MinChunkSize ?? 100
            },
            AiEnhancement = new AiEnhancementConfiguration
            {
                // AI ì¦ê°•ì€ êµ¬ì„±ì—ì„œ ì„¤ì •ëœ ê°’ ì‚¬ìš© (ê¸°ë³¸ê°’: false)
                Enabled = true, // í…ŒìŠ¤íŠ¸ë¥¼ ìœ„í•´ í™œì„±í™”
                EnableSummary = true,
                EnableMetadata = true
            }
        };

        // ProcessAsyncë¥¼ í˜¸ì¶œí•˜ì—¬ íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        var chunks = new List<WebContentChunk>();

        await foreach (var chunk in ProcessAsync(configuration, cancellationToken))
        {
            chunks.Add(chunk);
        }

        _logger.LogInformation("Completed processing URL {Url}: {ChunkCount} chunks generated",
            url, chunks.Count);

        return chunks.AsReadOnly();
    }

    public async Task<IReadOnlyDictionary<string, IReadOnlyList<WebContentChunk>>> ProcessUrlsBatchAsync(
        IEnumerable<string> urls,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        // Stub implementation
        await Task.CompletedTask;
        return new Dictionary<string, IReadOnlyList<WebContentChunk>>().AsReadOnly();
    }

    public async IAsyncEnumerable<WebContentChunk> ProcessWebsiteAsync(
        string startUrl,
        CrawlOptions? crawlOptions = null,
        ChunkingOptions? chunkingOptions = null,
        [System.Runtime.CompilerServices.EnumeratorCancellation] CancellationToken cancellationToken = default)
    {
        _logger.LogInformation("Processing website: {Url} (Dynamic: {Dynamic})",
            startUrl, crawlOptions?.UseDynamicRendering ?? false);

        // WebFluxConfiguration ìƒì„±
        var configuration = new WebFluxConfiguration
        {
            Crawling = new CrawlingConfiguration
            {
                StartUrls = new List<string> { startUrl },
                // CrawlOptionsê°€ ì œê³µë˜ê³  UseDynamicRenderingì´ trueë©´ Dynamic ì „ëµ ì‚¬ìš©
                Strategy = (crawlOptions?.UseDynamicRendering == true || crawlOptions?.Strategy == WebFlux.Core.Options.CrawlStrategy.Dynamic)
                    ? "Dynamic"
                    : "BreadthFirst",
                DefaultDelayMs = crawlOptions?.DelayMs ?? 0
            },
            Chunking = new ChunkingConfiguration
            {
                DefaultStrategy = chunkingOptions?.Strategy.ToString() ?? "Auto",
                MaxChunkSize = chunkingOptions?.MaxChunkSize ?? 1000,
                MinChunkSize = chunkingOptions?.MinChunkSize ?? 100
            },
            AiEnhancement = new AiEnhancementConfiguration
            {
                Enabled = false
            },
            Performance = new PerformanceConfiguration
            {
                MaxDegreeOfParallelism = 3
            }
        };

        // CrawlOptionsë¥¼ ì§ì ‘ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§
        var crawlStrategy = (crawlOptions?.UseDynamicRendering == true || crawlOptions?.Strategy == WebFlux.Core.Options.CrawlStrategy.Dynamic)
            ? WebFlux.Core.Options.CrawlStrategy.Dynamic
            : WebFlux.Core.Options.CrawlStrategy.BreadthFirst;

        var crawler = _serviceFactory.CreateCrawler(crawlStrategy);

        // CrawlOptions ì ìš©
        var effectiveCrawlOptions = crawlOptions ?? new CrawlOptions
        {
            MaxDepth = 1,
            MaxPages = 1,
            DelayMs = 0,
            TimeoutMs = 15000
        };

        _logger.LogInformation("Using crawler: {Strategy}, UseDynamicRendering: {Dynamic}",
            crawlStrategy, effectiveCrawlOptions.UseDynamicRendering);

        // í¬ë¡¤ë§ ë° ì²˜ë¦¬
        await foreach (var crawlResult in crawler.CrawlWebsiteAsync(startUrl, effectiveCrawlOptions, cancellationToken))
        {
            if (string.IsNullOrEmpty(crawlResult.Content))
            {
                _logger.LogWarning("Empty content from {Url}", crawlResult.Url);
                continue;
            }

            // ì½˜í…ì¸  ì¶”ì¶œ
            var extractor = _serviceFactory.CreateContentExtractor(crawlResult.ContentType ?? "text/html");
            var extracted = await extractor.ExtractAutoAsync(
                crawlResult.Content,
                crawlResult.Url,
                crawlResult.ContentType ?? "text/html",
                cancellationToken);

            if (string.IsNullOrEmpty(extracted.MainContent) && string.IsNullOrEmpty(extracted.Text))
            {
                _logger.LogWarning("No extractable content from {Url}", crawlResult.Url);
                continue;
            }

            // ì²­í‚¹
            var chunkingStrategy = _serviceFactory.CreateChunkingStrategy(
                (chunkingOptions?.Strategy ?? ChunkingStrategyType.Auto).ToString());

            var effectiveChunkingOptions = chunkingOptions ?? new ChunkingOptions
            {
                MaxChunkSize = 1000,
                ChunkOverlap = 50,
                PreserveHeaders = true
            };

            var chunks = await chunkingStrategy.ChunkAsync(
                extracted,
                effectiveChunkingOptions,
                cancellationToken);

            _logger.LogInformation("Generated {ChunkCount} chunks from {Url}", chunks.Count, crawlResult.Url);

            foreach (var chunk in chunks)
            {
                yield return chunk;
            }
        }
    }

    public async Task<IReadOnlyList<WebContentChunk>> ProcessHtmlAsync(
        string htmlContent,
        string sourceUrl,
        ChunkingOptions? chunkingOptions = null,
        CancellationToken cancellationToken = default)
    {
        // Stub implementation
        await Task.CompletedTask;
        return new List<WebContentChunk>().AsReadOnly();
    }

    public async IAsyncEnumerable<ProcessingProgress> MonitorProgressAsync(string jobId)
    {
        // Stub implementation
        await Task.CompletedTask;
        yield break;
    }

    public async Task<bool> CancelJobAsync(string jobId)
    {
        // Stub implementation
        await Task.CompletedTask;
        return true;
    }

    public async Task<ProcessingStatistics> GetStatisticsAsync()
    {
        // Stub implementation
        await Task.CompletedTask;
        return new ProcessingStatistics();
    }

    public IReadOnlyList<string> GetAvailableChunkingStrategies()
    {
        // Stub implementation
        return new List<string> { "FixedSize", "Paragraph", "Smart", "Semantic", "Auto", "MemoryOptimized" }.AsReadOnly();
    }
}

/// <summary>
/// ì²˜ë¦¬ ì‹œì‘ ì´ë²¤íŠ¸
/// </summary>
public class ProcessingStartedEvent : ProcessingEvent
{
    public override string EventType => "ProcessingStarted";
    public WebFluxConfiguration Configuration { get; set; } = new();
    public List<string> StartUrls { get; set; } = new();
}

/// <summary>
/// ì²˜ë¦¬ ì§„í–‰ë¥  ì´ë²¤íŠ¸
/// </summary>
public class ProcessingProgressEvent : ProcessingEvent
{
    public override string EventType => "ProcessingProgress";
    public int ProcessedCount { get; set; }
    public TimeSpan ElapsedTime { get; set; }
    public TimeSpan EstimatedRemaining { get; set; }
    public string CurrentStage { get; set; } = string.Empty;
}

/// <summary>
/// ì²˜ë¦¬ ì™„ë£Œ ì´ë²¤íŠ¸
/// </summary>
public class ProcessingCompletedEvent : ProcessingEvent
{
    public override string EventType => "ProcessingCompleted";
    public int ProcessedChunkCount { get; set; }
    public TimeSpan TotalProcessingTime { get; set; }
    public double AverageProcessingRate { get; set; }
}

/// <summary>
/// ì²˜ë¦¬ ì‹¤íŒ¨ ì´ë²¤íŠ¸
/// </summary>
public class ProcessingFailedEvent : ProcessingEvent
{
    public override string EventType => "ProcessingFailed";
    public string Error { get; set; } = string.Empty;
    public int ProcessedCount { get; set; }
}